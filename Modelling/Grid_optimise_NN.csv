parameters,RMSE,MAPE
layers: [16_12_8_4] learning rate: 1e-05 batch size: 64,634.2822528101854,0.10539368613813062
layers: [16_12_8_4] learning rate: 1e-05 batch size: 96,605.7822112840149,0.0935911775657271
layers: [16_12_8_4] learning rate: 1e-05 batch size: 128,594.8822260014655,0.0951448122257207
layers: [16_12_8_4] learning rate: 1e-05 batch size: 256,657.1426382851787,0.1028185742706744
layers: [16_12_8_4] learning rate: 0.0001 batch size: 64,616.8227879263608,0.10046652266787678
layers: [16_12_8_4] learning rate: 0.0001 batch size: 96,545.6300922640517,0.08254800408778357
layers: [16_12_8_4] learning rate: 0.0001 batch size: 128,434.28636243368084,0.06613190584527931
layers: [16_12_8_4] learning rate: 0.0001 batch size: 256,448.70626658395827,0.06867145149349668
layers: [16_12_8_4] learning rate: 0.001 batch size: 64,417.249869848076,0.06346205268700908
layers: [16_12_8_4] learning rate: 0.001 batch size: 96,482.92221083625424,0.07576468704335164
layers: [16_12_8_4] learning rate: 0.001 batch size: 128,402.3376244547381,0.060843207819283006
layers: [16_12_8_4] learning rate: 0.001 batch size: 256,439.10109007454065,0.06551639180794196
layers: [12_8_4] learning rate: 1e-05 batch size: 64,696.4580465359071,0.11427887577761513
layers: [12_8_4] learning rate: 1e-05 batch size: 96,607.3725971766363,0.09603401400453299
layers: [12_8_4] learning rate: 1e-05 batch size: 128,610.9390266356436,0.09784454822988113
layers: [12_8_4] learning rate: 1e-05 batch size: 256,883.2349024863804,0.13793186257432258
layers: [12_8_4] learning rate: 0.0001 batch size: 64,453.6734822515533,0.07011831250868465
layers: [12_8_4] learning rate: 0.0001 batch size: 96,442.3812864750311,0.06843685276134777
layers: [12_8_4] learning rate: 0.0001 batch size: 128,468.8914088064363,0.06938269788485747
layers: [12_8_4] learning rate: 0.0001 batch size: 256,549.8036331210149,0.08427186503326359
layers: [12_8_4] learning rate: 0.001 batch size: 64,417.92409631577914,0.06492866478169794
layers: [12_8_4] learning rate: 0.001 batch size: 96,483.991635491064,0.07384027611822215
layers: [12_8_4] learning rate: 0.001 batch size: 128,458.1582049924049,0.07353802867523745
layers: [12_8_4] learning rate: 0.001 batch size: 256,431.50516128345737,0.06484724849669479
layers: [8_4] learning rate: 1e-05 batch size: 64,546.9776932923227,0.08743212958860021
layers: [8_4] learning rate: 1e-05 batch size: 96,535.9537809405492,0.08445087200674373
layers: [8_4] learning rate: 1e-05 batch size: 128,611.074333453316,0.09575753291710258
layers: [8_4] learning rate: 1e-05 batch size: 256,693.5956559871795,0.10612723668189168
layers: [8_4] learning rate: 0.0001 batch size: 64,422.85693528634897,0.06624398571221712
layers: [8_4] learning rate: 0.0001 batch size: 96,509.53287389545363,0.07661127107399468
layers: [8_4] learning rate: 0.0001 batch size: 128,434.83313925372886,0.06634312224190814
layers: [8_4] learning rate: 0.0001 batch size: 256,426.4236425044821,0.0656578249572038
layers: [8_4] learning rate: 0.001 batch size: 64,434.4319814480018,0.06851328101499457
layers: [8_4] learning rate: 0.001 batch size: 96,449.2691958181925,0.06991488198558783
layers: [8_4] learning rate: 0.001 batch size: 128,389.92870631794506,0.05954738578046004
layers: [8_4] learning rate: 0.001 batch size: 256,425.5016624901174,0.06520365076369485
layers: [4] learning rate: 1e-05 batch size: 64,500.0669604592947,0.0799621613742716
layers: [4] learning rate: 1e-05 batch size: 96,527.5779653533529,0.08326352239818406
layers: [4] learning rate: 1e-05 batch size: 128,846.5033221480495,0.13362428955125888
layers: [4] learning rate: 1e-05 batch size: 256,583.9170671489927,0.09267351872239703
layers: [4] learning rate: 0.0001 batch size: 64,497.148611816556,0.07299812163858564
layers: [4] learning rate: 0.0001 batch size: 96,421.94590634435883,0.06735945171431619
layers: [4] learning rate: 0.0001 batch size: 128,436.9447094321564,0.0689142085325505
layers: [4] learning rate: 0.0001 batch size: 256,415.86988710680663,0.06563217972399865
layers: [4] learning rate: 0.001 batch size: 64,389.11919602971204,0.06129882922593801
layers: [4] learning rate: 0.001 batch size: 96,413.1479797618526,0.06492402393232337
layers: [4] learning rate: 0.001 batch size: 128,388.6158293576329,0.060337897945301096
layers: [4] learning rate: 0.001 batch size: 256,448.78003251241495,0.07127286662183627
layers: [16_16_12_8_4] learning rate: 1e-05 batch size: 64,728.2524900267138,0.1093277982903918
layers: [16_16_12_8_4] learning rate: 1e-05 batch size: 96,609.2290159283486,0.09635226382984816
layers: [16_16_12_8_4] learning rate: 1e-05 batch size: 128,604.2342325008674,0.09615525263807326
layers: [16_16_12_8_4] learning rate: 1e-05 batch size: 256,669.2651967630901,0.10379809619818497
layers: [16_16_12_8_4] learning rate: 0.0001 batch size: 64,457.99401801138174,0.06926695858149934
layers: [16_16_12_8_4] learning rate: 0.0001 batch size: 96,472.79457835832795,0.07103872940755221
layers: [16_16_12_8_4] learning rate: 0.0001 batch size: 128,509.09163495992306,0.08182995911875846
layers: [16_16_12_8_4] learning rate: 0.0001 batch size: 256,501.66497562493316,0.08015534202447432
layers: [16_16_12_8_4] learning rate: 0.001 batch size: 64,439.5566386197955,0.06624319927496705
layers: [16_16_12_8_4] learning rate: 0.001 batch size: 96,426.6226096056758,0.06627455937783461
layers: [16_16_12_8_4] learning rate: 0.001 batch size: 128,508.8593137373541,0.07540869156019235
layers: [16_16_12_8_4] learning rate: 0.001 batch size: 256,431.5287264392478,0.06579459140645741
layers: [16_12_12_8_4] learning rate: 1e-05 batch size: 64,604.9961733945537,0.09864506980366623
layers: [16_12_12_8_4] learning rate: 1e-05 batch size: 96,624.329891643997,0.09854666260055232
layers: [16_12_12_8_4] learning rate: 1e-05 batch size: 128,606.2683042075435,0.09705686787425018
layers: [16_12_12_8_4] learning rate: 1e-05 batch size: 256,879.4150381616264,0.13660029849507135
layers: [16_12_12_8_4] learning rate: 0.0001 batch size: 64,460.02272824877025,0.07370709113935842
layers: [16_12_12_8_4] learning rate: 0.0001 batch size: 96,554.8762650914956,0.08116394124775128
layers: [16_12_12_8_4] learning rate: 0.0001 batch size: 128,682.7569364676675,0.1027617402888674
layers: [16_12_12_8_4] learning rate: 0.0001 batch size: 256,538.0764006241917,0.08875952266440663
layers: [16_12_12_8_4] learning rate: 0.001 batch size: 64,501.30270479885746,0.07547482262302709
layers: [16_12_12_8_4] learning rate: 0.001 batch size: 96,449.39569978658784,0.07070895151602488
layers: [16_12_12_8_4] learning rate: 0.001 batch size: 128,437.73248977416046,0.064657555090316
layers: [16_12_12_8_4] learning rate: 0.001 batch size: 256,543.2810562027892,0.08035509012540815
layers: [4_4_4_4] learning rate: 1e-05 batch size: 64,671.9702464525978,0.10525480258454853
layers: [4_4_4_4] learning rate: 1e-05 batch size: 96,669.8683422035215,0.10626898844885713
layers: [4_4_4_4] learning rate: 1e-05 batch size: 128,653.4736215925532,0.102571300959588
layers: [4_4_4_4] learning rate: 1e-05 batch size: 256,681.4671245757413,0.10670350614712533
layers: [4_4_4_4] learning rate: 0.0001 batch size: 64,550.7314982852408,0.08583888503072547
layers: [4_4_4_4] learning rate: 0.0001 batch size: 96,494.4939084523751,0.07577229140625216
layers: [4_4_4_4] learning rate: 0.0001 batch size: 128,672.6845983734604,0.10749250363672337
layers: [4_4_4_4] learning rate: 0.0001 batch size: 256,664.7181872197717,0.10501180678972573
layers: [4_4_4_4] learning rate: 0.001 batch size: 64,515.1166338724252,0.07918026647124911
layers: [4_4_4_4] learning rate: 0.001 batch size: 96,557.8719660213364,0.0814897419212945
layers: [4_4_4_4] learning rate: 0.001 batch size: 128,596.1266413380679,0.09173795044138296
layers: [4_4_4_4] learning rate: 0.001 batch size: 256,497.84752145724605,0.07666786121030769
